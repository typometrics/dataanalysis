{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ab4ae3e3",
            "metadata": {},
            "source": [
                "## Imports\n",
                "\n",
                "**Environment Setup**: Select the `ragenv` kernel from the kernel picker (top-right corner of the notebook). The kernel has been registered and should be available in the list."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "0ea3d0f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from glob import glob\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Import custom modules\n",
                "import data_utils\n",
                "import conll_processing\n",
                "import validation"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "731e293a",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "a7dc29f2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths and constants\n",
                "UD_VERSION = \"2.17\"\n",
                "UD_DIR = f\"ud-treebanks-v{UD_VERSION}\"\n",
                "CREDENTIALS_FILE = \"typometrics-c4750cac2e21.json\"\n",
                "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1IP3ebsNNVAsQ5sxmBnfEAmZc4f0iotAL9hd4aqOOcEg/edit\"\n",
                "DATA_DIR = \"data\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25daf473",
            "metadata": {},
            "source": [
                "## 1. Download and Extract UD Treebanks\n",
                "\n",
                "Download the latest UD treebanks from the official repository if not already present."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "54bef594",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "UD treebanks v2.17 found at: /bigstorage/kim/typometrics/dataanalysis/ud-treebanks-v2.17\n"
                    ]
                }
            ],
            "source": [
                "# Check if already downloaded\n",
                "if not os.path.exists(UD_DIR):\n",
                "    print(f\"UD treebanks v{UD_VERSION} not found. Please download manually:\")\n",
                "    print(f\"https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5772\")\n",
                "    print(f\"Extract to: {os.path.abspath(UD_DIR)}\")\n",
                "else:\n",
                "    print(f\"UD treebanks v{UD_VERSION} found at: {os.path.abspath(UD_DIR)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "612fa0cf",
            "metadata": {},
            "source": [
                "## 2. Load CoNLL Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "ce9858f6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Excluding 2 treebanks: {'UD_Akkadian-RIAO', 'UD_Akkadian-PISANDUB'}\n",
                        "Languages represented: 186\n",
                        "Sample languages: ['abq', 'ab', 'af', 'aqz', 'sq', 'gsw', 'am', 'grc', 'hbo', 'apu']\n"
                    ]
                }
            ],
            "source": [
                "# Find all CoNLL-U files and group by language\n",
                "# This matches the original getAllConllFilesGroup() function\n",
                "langConllFiles = {}\n",
                "\n",
                "# Load excluded treebanks\n",
                "excluded_treebanks = set()\n",
                "if os.path.exists(\"data/excluded_treebanks.txt\"):\n",
                "    with open(\"data/excluded_treebanks.txt\", \"r\") as f:\n",
                "        excluded_treebanks = set(line.strip() for line in f if line.strip())\n",
                "print(f\"Excluding {len(excluded_treebanks)} treebanks: {excluded_treebanks}\")\n",
                "\n",
                "# List all directories in the UD folder\n",
                "doc_list = [d for d in sorted(os.listdir(UD_DIR)) if os.path.isdir(os.path.join(UD_DIR, d))]\n",
                "\n",
                "for doc_name in doc_list:\n",
                "    if doc_name in excluded_treebanks:\n",
                "        continue\n",
                "    doc_path = os.path.join(UD_DIR, doc_name)\n",
                "    # Find all .conllu files in this directory (excluding not-to-release)\n",
                "    conll_files = [\n",
                "        os.path.join(doc_path, f) \n",
                "        for f in os.listdir(doc_path) \n",
                "        if f.endswith(\".conllu\") and \"not-to-release\" not in doc_name\n",
                "    ]\n",
                "    \n",
                "    if conll_files:\n",
                "        # Extract language code from first filename (e.g., en_ewt-ud-train.conllu -> en)\n",
                "        first_file = os.path.basename(conll_files[0])\n",
                "        \n",
                "        # Special handling for fr_alts (Old/Middle French mixed)\n",
                "        if first_file.startswith(\"fr_alts\"):\n",
                "            lang_code = \"fr_alts\"\n",
                "        else:\n",
                "            lang_code = first_file.split('_', 1)[0].lower()\n",
                "        \n",
                "        # Add all files for this language\n",
                "        if lang_code not in langConllFiles:\n",
                "            langConllFiles[lang_code] = []\n",
                "        langConllFiles[lang_code].extend(conll_files)\n",
                "\n",
                "print(f\"Languages represented: {len(langConllFiles)}\")\n",
                "print(f\"Sample languages: {list(langConllFiles.keys())[:10]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ee018b9f",
            "metadata": {},
            "source": [
                "## 3. Load Google Sheets Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "57125120",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded sheets: ['my_language', 'language_to_group', 'appearance', 'all_languages_code']\n"
                    ]
                }
            ],
            "source": [
                "# Load Google Sheets data\n",
                "sheets_data = data_utils.load_google_sheets(CREDENTIALS_FILE, SPREADSHEET_URL)\n",
                "print(\"Loaded sheets:\", list(sheets_data['sheets'].keys()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "7a597703",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "my_language:\n",
                        "  code        displayName\n",
                        "0   ca            Catalan\n",
                        "1   cu  OldChurchSlavonic\n",
                        "2   nl              Dutch\n",
                        "3   el              Greek\n",
                        "4   ht            Haitian\n",
                        "\n",
                        "language_to_group:\n",
                        "    Language          Group     Genus Column 1    Simple Group Area\n",
                        "0      Abaza      Caucasian                          Caucasian    E\n",
                        "1     Abkhaz      Caucasian                          Caucasian    E\n",
                        "2  Afrikaans  Indo-European  Germanic            Indo-European   Af\n",
                        "3   Akkadian        Semitic                        Afroasiatic   ME\n",
                        "4    Akuntsu         Tupian                     South-American   SA\n",
                        "\n",
                        "appearance:\n",
                        "           Group Default Color\n",
                        "0         Italic         brown\n",
                        "1    Baltoslavic        purple\n",
                        "2       Germanic         olive\n",
                        "3  Indo-European     royalBlue\n",
                        "4   Austronesian     limeGreen\n",
                        "\n",
                        "all_languages_code:\n",
                        "  code   language\n",
                        "0   ab  Abkhazian\n",
                        "1   aa       Afar\n",
                        "2   af  Afrikaans\n",
                        "3   ak       Akan\n",
                        "4   sq   Albanian\n"
                    ]
                }
            ],
            "source": [
                "# Display sheet previews\n",
                "for sheet_name, df in sheets_data['dataframes'].items():\n",
                "    print(f\"\\n{sheet_name}:\")\n",
                "    print(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dc97250d",
            "metadata": {},
            "source": [
                "## 4. Create Language Mappings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "870bbf32",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total language codes available: 8041\n",
                        "Languages in our treebanks: 186\n",
                        "Languages with names: 186\n",
                        "Total language groups: 11\n",
                        "Groups with colors: 29\n"
                    ]
                }
            ],
            "source": [
                "# Create language mappings\n",
                "# Note: langNames comes from all_languages_code sheet (all ISO codes),\n",
                "# then overridden by my_language sheet (custom display names for languages with spaces)\n",
                "mappings = data_utils.create_language_mappings(sheets_data)\n",
                "\n",
                "# Filter to only languages in our treebanks\n",
                "all_langNames = mappings['langNames']\n",
                "langNames = {lang: all_langNames[lang] for lang in langConllFiles if lang in all_langNames}\n",
                "\n",
                "# Manually add fr_alts if not present\n",
                "if 'fr_alts' in langConllFiles and 'fr_alts' not in langNames:\n",
                "    langNames['fr_alts'] = \"French (Alternative)\"\n",
                "\n",
                "langnameGroup = mappings['langnameGroup']\n",
                "\n",
                "# Manually add group for fr_alts\n",
                "if 'French (Alternative)' not in langnameGroup:\n",
                "    langnameGroup['French (Alternative)'] = 'Indo-European'\n",
                "\n",
                "group2lang = mappings['group2lang']\n",
                "appearance_dict = mappings['appearance_dict']\n",
                "\n",
                "print(f\"Total language codes available: {len(all_langNames)}\")\n",
                "print(f\"Languages in our treebanks: {len(langConllFiles)}\")\n",
                "print(f\"Languages with names: {len(langNames)}\")\n",
                "print(f\"Total language groups: {len(set(langnameGroup.values()))}\")\n",
                "print(f\"Groups with colors: {len(appearance_dict)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fa64004b",
            "metadata": {},
            "source": [
                "## 5. Validate Language Codes\n",
                "\n",
                "Check which languages in our treebanks:\n",
                "- Have names with spaces (need custom display names in my_language sheet)\n",
                "- Are missing from the language code mapping"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "e41906ac",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Language with space:\n",
                        " fr_alts: French (Alternative)\n",
                        "Language to add: []\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Language code validation complete. Check Google Sheet column E for results.\n"
                    ]
                }
            ],
            "source": [
                "# Validate language codes\n",
                "my_language_sheet = sheets_data['sheets']['my_language']\n",
                "validation.validate_language_codes(langConllFiles, langNames, my_language_sheet)\n",
                "print(\"Language code validation complete. Check Google Sheet column E for results.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6b80d467",
            "metadata": {},
            "source": [
                "## 6. Validate Language Groups\n",
                "\n",
                "Check which languages in our treebanks are missing group assignments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "4a19f838",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0 language groups to add:\n",
                        " \n",
                        "['OK', '', '', '', '', '', '', '', '', '', '']\n",
                        "Language group validation complete. Check Google Sheet column H for results.\n"
                    ]
                }
            ],
            "source": [
                "# Validate language groups\n",
                "language_to_group_sheet = sheets_data['sheets']['language_to_group']\n",
                "validation.validate_language_groups(langConllFiles, langNames, langnameGroup, language_to_group_sheet)\n",
                "print(\"Language group validation complete. Check Google Sheet column H for results.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa00a74d",
            "metadata": {},
            "source": [
                "## 7. Compute Basic Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "db03cf47",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b342984f96d54ad891c336a59b22b6ed",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Computing statistics:   0%|          | 0/186 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   language   languageName           group  nConllFiles  nSentences  nTokens  \\\n",
                        "0       abq          Abaza       Caucasian            1          98     1240   \n",
                        "1        ab         Abkhaz       Caucasian            1        1316    14533   \n",
                        "2        af      Afrikaans   Indo-European            3        1936    55062   \n",
                        "3       aqz        Akuntsu  South-American            1         343     2762   \n",
                        "4        sq       Albanian   Indo-European            4         263     5270   \n",
                        "5       gsw    SwissGerman   Indo-European            2        1078    26213   \n",
                        "6        am        Amharic     Afroasiatic            1        1074    15932   \n",
                        "7       grc   AncientGreek   Indo-European            9       32585   540353   \n",
                        "8       hbo  AncientHebrew     Afroasiatic            3        5610   208302   \n",
                        "9       apu        Apurinã  South-American            1         165     1990   \n",
                        "10       ar         Arabic     Afroasiatic            7       28408  1259114   \n",
                        "11       hy       Armenian   Indo-European            6        8055   179244   \n",
                        "12      aii       Assyrian     Afroasiatic            1          57      568   \n",
                        "13       az    Azerbaijani          Turkic            1         148     1651   \n",
                        "14       bm        Bambara     Niger-Congo            1        1026    15909   \n",
                        "15       eu         Basque           Other            3        8995   139431   \n",
                        "16      bar       Bavarian   Indo-European            1        1070    21815   \n",
                        "17      bej           Beja     Afroasiatic            1         763    16184   \n",
                        "18       be     Belarusian   Indo-European            3       25233   431580   \n",
                        "19       bn        Bengali   Indo-European            1          56      544   \n",
                        "\n",
                        "    avgSentenceLength  \n",
                        "0           12.653061  \n",
                        "1           11.043313  \n",
                        "2           28.441116  \n",
                        "3            8.052478  \n",
                        "4           20.038023  \n",
                        "5           24.316327  \n",
                        "6           14.834264  \n",
                        "7           16.582876  \n",
                        "8           37.130481  \n",
                        "9           12.060606  \n",
                        "10          44.322515  \n",
                        "11          22.252514  \n",
                        "12           9.964912  \n",
                        "13          11.155405  \n",
                        "14          15.505848  \n",
                        "15          15.500945  \n",
                        "16          20.387850  \n",
                        "17          21.211009  \n",
                        "18          17.103793  \n",
                        "19           9.714286  \n"
                    ]
                }
            ],
            "source": [
                "# Compute basic statistics for each language\n",
                "stats_df = validation.compute_basic_statistics(langConllFiles, langNames, langnameGroup)\n",
                "print(stats_df.head(20))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "bc861fec",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Summary:\n",
                        "       nConllFiles     nSentences       nTokens  avgSentenceLength\n",
                        "count   186.000000     186.000000  1.860000e+02         186.000000\n",
                        "mean      3.677419   12430.387097  2.474260e+05          17.462119\n",
                        "std       4.472526   33585.448798  6.755955e+05           7.234554\n",
                        "min       1.000000       8.000000  8.700000e+01           6.220447\n",
                        "25%       1.000000     248.750000  3.422000e+03          11.972462\n",
                        "50%       2.000000    1221.000000  1.828150e+04          15.634257\n",
                        "75%       4.000000    5938.000000  1.449330e+05          22.034783\n",
                        "max      29.000000  253797.000000  5.285270e+06          44.322515\n"
                    ]
                }
            ],
            "source": [
                "# Summary statistics\n",
                "print(\"\\nSummary:\")\n",
                "print(stats_df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32f04927",
            "metadata": {},
            "source": [
                "## 8. Create Short CoNLL Files\n",
                "\n",
                "Split large CoNLL files into chunks of 10,000 sentences for parallel processing.\n",
                "This takes 33 seconds on Calcul."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "d5a871ff",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating short CoNLL files (this may take a while)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing languages: 100%|██████████| 186/186 [00:30<00:00,  6.05it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 808 short CoNLL files in 2.17_short\n",
                        "Short files created.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Create short files if not already present\n",
                "short_dir = f\"{UD_DIR}_short\"\n",
                "if not os.path.exists(short_dir):\n",
                "    print(\"Creating short CoNLL files (this may take a while)...\")\n",
                "    conll_processing.make_shorter_conll_files(langConllFiles, UD_VERSION)\n",
                "    print(\"Short files created.\")\n",
                "else:\n",
                "    print(f\"Short files already exist in {short_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "28a2d25a",
            "metadata": {},
            "source": [
                "## 9. Read Short CoNLL Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "a8a70a6c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading short CoNLL files...\n",
                        "Found 808 short CoNLL files in 2.17_short\n",
                        "Total short files: 808\n",
                        "Languages with short files: 186\n"
                    ]
                }
            ],
            "source": [
                "# Read short files\n",
                "print(\"Reading short CoNLL files...\")\n",
                "langShortConllFiles, allshortconll = conll_processing.read_shorter_conll_files(langConllFiles, UD_VERSION)\n",
                "\n",
                "total_short_files = sum(len(files) for files in langShortConllFiles.values())\n",
                "print(f\"Total short files: {total_short_files}\")\n",
                "print(f\"Languages with short files: {len(langShortConllFiles)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6b2ee000",
            "metadata": {},
            "source": [
                "## 10. Export Metadata\n",
                "\n",
                "Save all metadata and file lists for use in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "78e244e1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved metadata to data/metadata.pkl\n",
                        "Metadata saved to data/metadata.pkl\n"
                    ]
                }
            ],
            "source": [
                "# Save metadata\n",
                "metadata = {\n",
                "    'langConllFiles': langConllFiles,\n",
                "    'langShortConllFiles': langShortConllFiles,\n",
                "    'langNames': langNames,\n",
                "    'langnameGroup': langnameGroup,\n",
                "    'group2lang': group2lang,\n",
                "    'appearance_dict': appearance_dict,\n",
                "    'ud_version': UD_VERSION\n",
                "}\n",
                "\n",
                "data_utils.save_metadata(metadata, os.path.join(DATA_DIR, 'metadata.pkl'))\n",
                "print(f\"Metadata saved to {DATA_DIR}/metadata.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b7910d59",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook has:\n",
                "- ✅ Loaded UD treebanks v2.17\n",
                "- ✅ Connected to Google Sheets for language metadata\n",
                "- ✅ Validated language codes and groups\n",
                "- ✅ Computed basic statistics (files, sentences, tokens)\n",
                "- ✅ Created short CoNLL files for parallel processing\n",
                "- ✅ Exported metadata for downstream notebooks\n",
                "\n",
                "**Next step**: Run `02_dependency_analysis.ipynb` to compute dependency size metrics."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "46c08ac7",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "75f324a2",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ragenv",
            "language": "python",
            "name": "ragenv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
