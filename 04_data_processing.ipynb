{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b5e44a",
   "metadata": {},
   "source": [
    "# 4. Factor & Table Computation\n",
    "\n",
    "**Summary**: Computes derived metrics like Hierarchical Complexity Scores (HCS), growth factors, and generates the Verb-Centered Tables.\n",
    "\n",
    "**Key Steps**:\n",
    "1. Compute HCS factors (ratios between adjacent positions).\n",
    "2. Generate Verb-Centered **Helix** Tables (incorporating growth factors).\n",
    "3. Merge Head-Initiality data.\n",
    "\n",
    "**Inputs**:\n",
    "- `data/all_langs_average_sizes.pkl`\n",
    "- `data/sentence_disorder_percentages.pkl` (if available)\n",
    "\n",
    "**Outputs**:\n",
    "- `data/hcs_factors.csv`\n",
    "- `data/verb_centered_table.txt`\n",
    "- `data/verb_centered_table_with_factors.tsv`\n",
    "\n",
    "**Runtime**: ~10-20 seconds\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f011922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'verb_centered_analysis' from '/bigstorage/kim/typometrics/dataanalysis/verb_centered_analysis.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from importlib import reload\n",
    "\n",
    "# Custom modules\n",
    "import data_utils\n",
    "import compute_factors\n",
    "import verb_centered_analysis\n",
    "\n",
    "# Reload to ensure latest changes are picked up\n",
    "reload(compute_factors)\n",
    "reload(verb_centered_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "OUTPUT_DIR = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load_metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata from data/metadata.pkl\n",
      "Loaded metadata for 187 languages\n"
     ]
    }
   ],
   "source": [
    "metadata = data_utils.load_metadata(os.path.join(DATA_DIR, 'metadata.pkl'))\n",
    "langNames = metadata['langNames']\n",
    "langnameGroup = metadata['langnameGroup']\n",
    "\n",
    "print(f\"Loaded metadata for {len(langNames)} languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded average sizes for 185 languages\n"
     ]
    }
   ],
   "source": [
    "# Load average sizes\n",
    "with open(os.path.join(DATA_DIR, 'all_langs_average_sizes.pkl'), 'rb') as f:\n",
    "    all_langs_average_sizes_filtered = pickle.load(f)\n",
    "\n",
    "# Save filtered data for notebook 05 (if needed by downstream)\n",
    "with open(os.path.join(DATA_DIR, 'all_langs_average_sizes_filtered.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_langs_average_sizes_filtered, f)\n",
    "\n",
    "print(f\"Loaded average sizes for {len(all_langs_average_sizes_filtered)} languages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hcs_factors",
   "metadata": {},
   "source": [
    "## 2. Compute HCS Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compute_hcs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed HCS factors for 170 languages\n",
      "    language_code  language_name           group  right_1_totright_2  \\\n",
      "80             ko         Korean           Other            1.824906   \n",
      "116           pad        Paumarí  South-American            2.139826   \n",
      "146            tn         Tswana     Niger-Congo            2.213364   \n",
      "137           ssp    SpanishSign   Indo-European            1.525921   \n",
      "144           qte  TeluguEnglish       Dravidian            1.000000   \n",
      "\n",
      "     right_2_totright_2  hcs_factor  \n",
      "80             1.031262    0.565104  \n",
      "116            1.414214    0.660901  \n",
      "146            1.778279    0.803428  \n",
      "137            1.389013    0.910278  \n",
      "144            1.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "hcs_df = compute_factors.compute_hcs_factors(\n",
    "    all_langs_average_sizes_filtered, \n",
    "    langNames, \n",
    "    langnameGroup\n",
    ")\n",
    "\n",
    "print(f\"Computed HCS factors for {len(hcs_df)} languages\")\n",
    "print(hcs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save_hcs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HCS factors to data/hcs_factors.csv\n"
     ]
    }
   ],
   "source": [
    "hcs_path = os.path.join(OUTPUT_DIR, 'hcs_factors.csv')\n",
    "hcs_df.to_csv(hcs_path, index=False)\n",
    "print(f\"Saved HCS factors to {hcs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verb_centered",
   "metadata": {},
   "source": [
    "## 3. Helix tables (Verb-Centered Constituent Size Analysis)\n",
    "\n",
    "Verb-Centered Constituent Size Analysis = a helix table with consituent size averages X per construction of dependents to the right and the left of the verb: \n",
    "      VXXXX\n",
    "      VXXX\n",
    "      VXX\n",
    "      VX\n",
    "     XV\n",
    "    XXV\n",
    "   XXXV\n",
    "  XXXXV\n",
    "\n",
    "This table shold come in multiple options:\n",
    "\n",
    "1. Simple: just showing the average constituent size X per construction of dependents to the right and the left of the verb\n",
    "2. Horizontal Growth: inbetween two Xs, the factor of growth going from left to right\n",
    "3. Horizontal Growth: inbetween two Xs, the factor of growth going from right to left\n",
    "4. Diagonal Growth added as an extra line: Going up right, the growth factor going between the last X of one to the next constructio, such as between the last X of VXXX to the last X of VXXXX etc.\n",
    "5. The same as 4 but going down left.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compute_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verb-Centered Constituent Size Analysis\n",
    "position_averages = verb_centered_analysis.compute_average_sizes_table(all_langs_average_sizes_filtered)\n",
    "# Table saved in mass generation step below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456c761",
   "metadata": {},
   "source": [
    "## 4. Generate Helix Tables (Mass Generation)\n",
    "\n",
    "Generate helix tables for all languages, families, and order types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8625d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## OPTIONAL: Compute Disorder Percentages\n",
    "# ## Uncomment to compute disorder statistics\n",
    "\n",
    "# import compute_disorder\n",
    "# from importlib import reload\n",
    "# reload(compute_disorder)\n",
    "\n",
    "# # Compute disorder statistics with granular stats if available\n",
    "# import pickle\n",
    "\n",
    "# # Load granular ordering stats\n",
    "# ordering_stats = {}\n",
    "# disorder_pct_path = 'data/sentence_disorder_percentages.pkl'\n",
    "# if os.path.exists(disorder_pct_path):\n",
    "#     with open(disorder_pct_path, 'rb') as f:\n",
    "#         ordering_stats = pickle.load(f)\n",
    "#     print(f\"Loaded granular ordering stats for {len(ordering_stats)} languages\")\n",
    "\n",
    "# disorder_df, disorder_percentages = compute_disorder.compute_disorder_statistics(\n",
    "#     all_langs_average_sizes_filtered,\n",
    "#     langNames,\n",
    "#     langnameGroup,\n",
    "#     ordering_stats=ordering_stats\n",
    "# )\n",
    "# print(f\"Computed disorder for {len(disorder_df)} languages\")\n",
    "# print(\"\\nDisorder percentages by configuration:\")\n",
    "# for (side, tot), pct in sorted(disorder_percentages.items()):\n",
    "#     if pct is not None:\n",
    "#         print(f\"  {side} tot={tot}: {pct:.1f}% disordered\")\n",
    "\n",
    "# print(\"\\n\", disorder_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfb224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Mass Table Generation ---\n",
      "Loaded Ordering Stats (Triplets) successfully.\n",
      "Loaded VO/OV classifications.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Global Table...\n",
      "Generating Family Tables (10 families)...\n",
      "Generating Individual Language Tables...\n",
      "Calculating disorder metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:00<00:00, 708.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Completed. Tables saved to data/tables/ ---\n",
      "\n",
      "Extracted and saved disorder metrics for 185 languages to data/disorder_extreme_aggregates.csv\n",
      "\n",
      "Sample disorder metrics:\n",
      "  language_code  language_name           group  left_tot_2_disordered  \\\n",
      "0           abq          Abaza       Caucasian               0.604167   \n",
      "1            ab         Abkhaz       Caucasian               0.629380   \n",
      "2            af      Afrikaans   Indo-European               0.765786   \n",
      "3           aqz        Akuntsu  South-American               0.884615   \n",
      "4            sq       Albanian   Indo-European               0.637168   \n",
      "5           gsw    SwissGerman   Indo-European               0.819608   \n",
      "6            am        Amharic     Afroasiatic               0.422430   \n",
      "7           grc   AncientGreek   Indo-European               0.810120   \n",
      "8           hbo  AncientHebrew     Afroasiatic               0.352941   \n",
      "9           apu        Apurinã  South-American               0.789474   \n",
      "\n",
      "   left_tot_3_disordered  left_tot_5_disordered  xvx_tot_2_disordered  \\\n",
      "0               0.760870               1.000000              0.818182   \n",
      "1               0.634454               0.750000              0.507614   \n",
      "2               0.723390               0.725000              0.842835   \n",
      "3                    NaN                    NaN              0.826923   \n",
      "4               0.826923               0.750000              0.885246   \n",
      "5               0.752044               0.774390              0.863158   \n",
      "6               0.504525               0.666667              0.268908   \n",
      "7               0.814360               0.782039              0.808047   \n",
      "8               0.549180               0.500000              0.718602   \n",
      "9               0.857143                    NaN              0.844828   \n",
      "\n",
      "   left_tot_4_disordered  right_tot_2_disordered  right_tot_3_disordered  ...  \\\n",
      "0               0.466667                0.750000                     NaN  ...   \n",
      "1               0.672414                0.580645                1.000000  ...   \n",
      "2               0.696970                0.318653                0.420561  ...   \n",
      "3                    NaN                1.000000                     NaN  ...   \n",
      "4               0.600000                0.207692                0.288462  ...   \n",
      "5               0.740741                0.305195                0.419355  ...   \n",
      "6               0.530612                0.976608                0.869565  ...   \n",
      "7               0.803932                0.356631                0.424746  ...   \n",
      "8               0.416667                0.306854                0.354262  ...   \n",
      "9                    NaN                0.560000                     NaN  ...   \n",
      "\n",
      "   right_tot_21_disordered  right_tot_15_disordered  left_tot_15_disordered  \\\n",
      "0                      NaN                      NaN                     NaN   \n",
      "1                      NaN                      NaN                     NaN   \n",
      "2                      NaN                      NaN                     NaN   \n",
      "3                      NaN                      NaN                     NaN   \n",
      "4                      NaN                      NaN                     NaN   \n",
      "5                      NaN                      NaN                     NaN   \n",
      "6                      NaN                      NaN                     NaN   \n",
      "7                      NaN                      NaN                     NaN   \n",
      "8                      NaN                      NaN                     NaN   \n",
      "9                      NaN                      NaN                     NaN   \n",
      "\n",
      "   right_tot_16_disordered  right_tot_18_disordered  total_disordered  \\\n",
      "0                      NaN                      NaN          4.399885   \n",
      "1                      NaN                      NaN          5.574507   \n",
      "2                      NaN                      NaN          8.718194   \n",
      "3                      NaN                      NaN          2.711538   \n",
      "4                      NaN                      NaN          5.345491   \n",
      "5                      NaN                      NaN          7.905442   \n",
      "6                      NaN                      NaN          6.239315   \n",
      "7                      NaN                      NaN         11.408834   \n",
      "8                      NaN                      NaN          4.996145   \n",
      "9                      NaN                      NaN          3.051444   \n",
      "\n",
      "   right_extreme_disorder  left_extreme_disorder  right_extreme_diag_factor  \\\n",
      "0                     NaN               0.466667                        NaN   \n",
      "1                     NaN               0.672414                        NaN   \n",
      "2                0.600000               0.696970                   0.912193   \n",
      "3                     NaN                    NaN                        NaN   \n",
      "4                0.333333               0.600000                   2.007977   \n",
      "5                0.500000               0.740741                   1.224747   \n",
      "6                0.750000               0.530612                   1.213820   \n",
      "7                0.496509               0.803932                   1.092064   \n",
      "8                0.432361               0.416667                   1.201321   \n",
      "9                     NaN                    NaN                        NaN   \n",
      "\n",
      "   left_extreme_diag_factor  \n",
      "0                      None  \n",
      "1                      None  \n",
      "2                      None  \n",
      "3                      None  \n",
      "4                      None  \n",
      "5                      None  \n",
      "6                      None  \n",
      "7                      None  \n",
      "8                      None  \n",
      "9                      None  \n",
      "\n",
      "[10 rows x 45 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import verb_centered_analysis\n",
    "\n",
    "# Reload the module to use the latest table formatting logic\n",
    "reload(verb_centered_analysis)\n",
    "\n",
    "OUTPUT_TABLE_DIR = os.path.join(DATA_DIR, 'tables')\n",
    "\n",
    "print(\"--- Starting Mass Table Generation ---\")\n",
    "\n",
    "# 1. Load Average Sizes (should already be loaded from previous cells)\n",
    "if 'all_langs_average_sizes_filtered' not in locals():\n",
    "    avg_path = os.path.join(DATA_DIR, 'all_langs_average_sizes_filtered.pkl')\n",
    "    if os.path.exists(avg_path):\n",
    "        with open(avg_path, 'rb') as f:\n",
    "            all_langs_average_sizes_filtered = pickle.load(f)\n",
    "        print(\"Loaded average sizes from disk.\")\n",
    "    else:\n",
    "        print(\"ERROR: 'all_langs_average_sizes_filtered' not found. Run previous cells first.\")\n",
    "\n",
    "# 2. Load Ordering Statistics (optional - for ordering triples)\n",
    "ordering_stats = {}\n",
    "disorder_path = os.path.join(DATA_DIR, 'sentence_disorder_percentages.pkl')\n",
    "\n",
    "if os.path.exists(disorder_path):\n",
    "    with open(disorder_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        \n",
    "    # Check if the data is in the new format (triplet counts)\n",
    "    sample_lang = next(iter(loaded_data))\n",
    "    sample_keys = list(loaded_data[sample_lang].keys()) if loaded_data[sample_lang] else []\n",
    "    \n",
    "    if sample_keys and len(sample_keys[0]) == 3:\n",
    "        print(\"Loaded Ordering Stats (Triplets) successfully.\")\n",
    "        ordering_stats = loaded_data\n",
    "    else:\n",
    "        print(\"WARNING: Loaded data seems to be in OLD format. Triples cannot be shown.\")\n",
    "else:\n",
    "    print(f\"WARNING: {disorder_path} not found. Tables will be generated without triples.\")\n",
    "\n",
    "# 3. Load VO/OV Data (optional - for order-based tables)\n",
    "vo_data = {}\n",
    "vo_path = os.path.join(DATA_DIR, 'vo_vs_hi_scores.csv')\n",
    "if os.path.exists(vo_path):\n",
    "    vo_df = pd.read_csv(vo_path)\n",
    "    for _, row in vo_df.iterrows():\n",
    "        vo_data[row['language_code']] = row.to_dict()\n",
    "    print(\"Loaded VO/OV classifications.\")\n",
    "\n",
    "# 4. Generate Tables AND Extract Disorder Metrics\n",
    "# This will generate: Global, Individual, Family-based, and Order-based tables.\n",
    "# Also extracts disorder extreme aggregate percentages for each language\n",
    "disorder_df = verb_centered_analysis.generate_mass_tables(\n",
    "    all_langs_average_sizes_filtered,\n",
    "    ordering_stats,\n",
    "    metadata,  # Ensure metadata is loaded (from Cell 6)\n",
    "    vo_data=vo_data,\n",
    "    output_dir=OUTPUT_TABLE_DIR,\n",
    "    arrow_direction='left_to_right',\n",
    "    extract_disorder_metrics=True\n",
    ")\n",
    "\n",
    "print(f\"--- Completed. Tables saved to {OUTPUT_TABLE_DIR}/ ---\")\n",
    "\n",
    "# Save disorder metrics if extracted\n",
    "if disorder_df is not None and len(disorder_df) > 0:\n",
    "    disorder_csv_path = os.path.join(DATA_DIR, 'disorder_extreme_aggregates.csv')\n",
    "    disorder_df.to_csv(disorder_csv_path, index=False)\n",
    "    print(f\"\\nExtracted and saved disorder metrics for {len(disorder_df)} languages to {disorder_csv_path}\")\n",
    "    print(\"\\nSample disorder metrics:\")\n",
    "    print(disorder_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769ec72",
   "metadata": {},
   "source": [
    "## 5. Conll Configuration Example Creation\n",
    "\n",
    "Generate interactive HTML visualizations of verb configurations from examples collected during data extraction.\n",
    "\n",
    "**Note**: Examples are automatically collected by `run_data_extraction.py` using the same constraints as constituent size computation (same dependency relations, bastard inclusion, etc.). This ensures consistency and avoids duplicate CoNLL file parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4831de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HTML visualizations from saved configuration examples...\n",
      "============================================================\n",
      "Loading configuration examples...\n",
      "Loading metadata...\n",
      "Loading position counts...\n",
      "Loaded position counts for 185 languages\n",
      "Generating HTML for 185 languages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating HTML files: 100%|██████████| 185/185 [00:05<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating index...\n",
      "Generated index at html_examples/index.html\n",
      "Done! HTML files saved to html_examples/\n",
      "\n",
      "============================================================\n",
      "Configuration Examples Generated Successfully!\n",
      "============================================================\n",
      "Output directory: html_examples/\n",
      "Open html_examples/index.html to browse examples\n",
      "\n",
      "Features:\n",
      "  • Interactive dependency trees with reactive-dep-tree\n",
      "  • Verbs highlighted in red, dependents in green\n",
      "  • Organized by language with 3-column layout\n",
      "  • Same constraints as constituent size computation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate HTML visualizations from configuration examples collected during data extraction\n",
    "# Examples are automatically collected in run_data_extraction.py using the same constraints\n",
    "# as constituent size computation (same dependency relations, bastard inclusion, etc.)\n",
    "\n",
    "import generate_html_examples\n",
    "from importlib import reload\n",
    "reload(generate_html_examples)\n",
    "\n",
    "print(\"Generating HTML visualizations from saved configuration examples...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if examples have been collected\n",
    "examples_path = os.path.join(DATA_DIR, 'all_config_examples.pkl')\n",
    "if not os.path.exists(examples_path):\n",
    "    print(f\"ERROR: Configuration examples not found at {examples_path}\")\n",
    "    print(\"\\nPlease run data extraction first:\")\n",
    "    print(\"  python3 run_data_extraction.py\")\n",
    "    print(\"\\nThis will collect examples during the main processing pipeline.\")\n",
    "else:\n",
    "    # Generate HTML from saved examples\n",
    "    output_dir = 'html_examples'\n",
    "    generate_html_examples.generate_all_html(\n",
    "        data_dir=DATA_DIR,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Configuration Examples Generated Successfully!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Output directory: {output_dir}/\")\n",
    "    print(f\"Open {output_dir}/index.html to browse examples\")\n",
    "    print(f\"\\nFeatures:\")\n",
    "    print(f\"  • Interactive dependency trees with reactive-dep-tree\")\n",
    "    print(f\"  • Verbs highlighted in red, dependents in green\")\n",
    "    print(f\"  • Organized by language with 3-column layout\")\n",
    "    print(f\"  • Same constraints as constituent size computation\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extract_rhapsodie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files in /bigstorage/kim/typometrics/dataanalysis/ud-treebanks-v2.17/UD_French-Rhapsodie/\n",
      "Processing with MAX_EXAMPLES=1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 3/3 [00:00<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Generating HTML...\n",
      "Done! Generated 34 configuration files with 3808 total examples.\n",
      "Output directory: /bigstorage/kim/typometrics/dataanalysis/html_examples/French_Rhapsodie_UD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run extraction for Rhapsodie Treebank (Custom Extraction)\n",
    "# This uses the dedicated extraction script to process Rhapsodie files directly\n",
    "\n",
    "import extract_treebank_configs\n",
    "from importlib import reload\n",
    "reload(extract_treebank_configs)\n",
    "\n",
    "# Run extraction for Rhapsodie\n",
    "extract_treebank_configs.main_func(\n",
    "    input_dir=\"/bigstorage/kim/typometrics/dataanalysis/ud-treebanks-v2.17/UD_French-Rhapsodie/\",\n",
    "    output_dir=\"html_examples\",\n",
    "    treebank_name=\"French_Rhapsodie_UD\",\n",
    "    max_examples=1_000_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f702c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using language: en\n",
      "Diag R2-1                                                                                                                     ×1.11↗                                                                                 \n",
      "R tot=1                                                                                                  V        3.349                                                                                  [GM: 3.349 | N=72090]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "X V X                                                                                       1.258   ×2.80→ (<74=20>6)   3.519                                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "L tot=1                                                                                     1.273        V                                                                                               [GM: 1.273 | N=54183]\n",
      "Diag L2-1                                                                         ×1.08↗                                                                                                                             \n",
      "L tot=2                                                               1.180   ×1.32→ (<28=62>9)   1.563        V                                                                                               [GM: 1.358 | N=23303 | Slope: +0.38]\n",
      "Diag L3-2                                                   ×1.06↗                   ×1.01↗                                                                                                                             \n",
      "L tot=3                                         1.117   ×1.38→ (<29=66>6)   1.545   ×1.23→ (<34=45>22)   1.904        V                                                                                               [GM: 1.486 | N=3342 | Slope: +0.39]\n",
      "Diag L4-3                             ×1.01↗                   ×1.06↗                   ×0.91↗                                                                                                                             \n",
      "L tot=4                   1.111   ×1.31→ (<27=66>7)   1.456   ×1.43→ (<40=45>15)   2.086   ×0.96→ (<32=33>36)   2.011        V                                                                                               [GM: 1.614 | N=347 | Slope: +0.33]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "M Diag Left     ×1.05↗        1.17       ×1.04↗        1.52       ×0.91↗        1.99                                                                                                                                       \n",
      "M Vert Left               1.111       ×1.15→       1.275       ×1.22→       1.561       ×1.06→       1.661                                                                                                                 \n",
      "Agg L First→                                                                                                                                                                                             [(<29.1=59.8>11.1) N=26992]\n"
     ]
    }
   ],
   "source": [
    "# Test updated table with all fixes\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Reload all verb-centered modules\n",
    "if 'verb_centered_model' in sys.modules:\n",
    "    importlib.reload(sys.modules['verb_centered_model'])\n",
    "if 'verb_centered_computations' in sys.modules:\n",
    "    importlib.reload(sys.modules['verb_centered_computations'])\n",
    "if 'verb_centered_layout' in sys.modules:\n",
    "    importlib.reload(sys.modules['verb_centered_layout'])\n",
    "if 'verb_centered_builder' in sys.modules:\n",
    "    importlib.reload(sys.modules['verb_centered_builder'])\n",
    "if 'verb_centered_formatters' in sys.modules:\n",
    "    importlib.reload(sys.modules['verb_centered_formatters'])\n",
    "if 'verb_centered_analysis' in sys.modules:\n",
    "    importlib.reload(sys.modules['verb_centered_analysis'])\n",
    "\n",
    "from verb_centered_analysis import create_verb_centered_table\n",
    "from verb_centered_model import TableConfig\n",
    "from verb_centered_formatters import TextTableFormatter\n",
    "\n",
    "config_debug = TableConfig(\n",
    "    show_horizontal_factors=True,\n",
    "    show_diagonal_factors=True,\n",
    "    show_ordering_triples=True,\n",
    "    show_row_averages=True,\n",
    "    show_marginal_means=True,\n",
    "    arrow_direction='left_to_right'\n",
    ")\n",
    "\n",
    "# For testing, we need language-specific data\n",
    "# Get English data if available\n",
    "test_lang = 'en'  # Try different language codes\n",
    "english_position_avg = None\n",
    "english_ordering = None\n",
    "\n",
    "for lang_code in all_langs_average_sizes_filtered.keys():\n",
    "    if lang_code.startswith('en'):\n",
    "        english_position_avg = all_langs_average_sizes_filtered[lang_code]\n",
    "        english_ordering = ordering_stats.get(lang_code, {})\n",
    "        print(f\"Using language: {lang_code}\")\n",
    "        break\n",
    "\n",
    "# Fallback to global if no English found\n",
    "if english_position_avg is None:\n",
    "    print(\"No English data found, using global averages (no ordering triples)\")\n",
    "    english_position_avg = position_averages\n",
    "    english_ordering = None\n",
    "\n",
    "# Create table structure\n",
    "table_struct = create_verb_centered_table(\n",
    "    position_averages=english_position_avg,\n",
    "    ordering_stats=english_ordering,\n",
    "    hcs_row=None,\n",
    "    config=config_debug,\n",
    "    output_format='struct'\n",
    ")\n",
    "\n",
    "# Format as text\n",
    "formatter = TextTableFormatter(table_struct)\n",
    "table_txt = formatter.format()\n",
    "\n",
    "# Print only lower half to check\n",
    "lines = table_txt.split('\\n')\n",
    "print('\\n'.join(lines[10:]))  # Skip upper half to focus on lower half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b26681c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No English ordering stats found\n",
      "Available languages: ['abq', 'ab', 'af', 'aqz', 'sq']\n"
     ]
    }
   ],
   "source": [
    "# Check ordering_stats for English\n",
    "if 'en_ewt' in ordering_stats:\n",
    "    print(\"English ordering stats found:\")\n",
    "    for key, value in sorted(ordering_stats['en_ewt'].items()):\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"No English ordering stats found\")\n",
    "    print(f\"Available languages: {list(ordering_stats.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2030de2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_averages type: <class 'dict'>\n",
      "First few keys: ['left_1', 'left_1_totleft_1', 'average_totleft_1', 'left_2', 'left_2_totleft_2', 'left_1_totleft_2', 'average_totleft_2', 'right_1', 'right_1_totright_1', 'average_totright_1']\n"
     ]
    }
   ],
   "source": [
    "# Check what position_averages contains\n",
    "print(\"position_averages type:\", type(position_averages))\n",
    "if isinstance(position_averages, dict):\n",
    "    print(\"First few keys:\", list(position_averages.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "197e719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row                         L4                    L3                    L2                    L1         V          R1                    R2                    R3                    R4                 [GM | N | Slope]\n",
      "M Vert Right                                                                                                      2.032                 2.765                 3.698                 5.257                         \n",
      "M Diag Right                                                                                                                             1.99       ×1.33↗        2.33       ×1.31↗        4.17       ×1.16↗               \n",
      "Agg R Last→                                                                                                                                                                                              [(<68.4=16.4>15.2) N=33542]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "R tot=4                                                                                                  V        1.712   ×1.34→ (<47=32>21)   2.288   ×1.29→ (<50=22>29)   2.953   ×1.78→ (<66=14>21)   5.257                [GM: 2.792 | N=546 | Slope: +1.13]\n",
      "Diag R4-3                                                                                                                     ×1.33↗                   ×1.19↗                   ×1.14↗                                     \n",
      "R tot=3                                                                                                  V        1.726   ×1.44→ (<53=27>20)   2.491   ×1.86→ (<69=13>18)   4.631                                      [GM: 2.710 | N=4557 | Slope: +1.45]\n",
      "Diag R3-2                                                                                                                     ×1.44↗                   ×1.25↗                                                           \n",
      "R tot=2                                                                                                  V        1.724   ×2.15→ (<68=17>15)   3.709                                                            [GM: 2.529 | N=28439 | Slope: +1.98]\n",
      "Diag R2-1                                                                                                                     ×1.11↗                                                                                 \n",
      "R tot=1                                                                                                  V        3.349                                                                                  [GM: 3.349 | N=72090]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "X V X                                                                                       1.258   ×2.80→ (<74=20>6)   3.519                                                                                           \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "L tot=1                                                                                     1.273        V                                                                                               [GM: 1.273 | N=54183]\n",
      "Diag L2-1                                                                         ×1.08↗                                                                                                                             \n",
      "L tot=2                                                               1.180   ×1.32→ (<28=62>9)   1.563        V                                                                                               [GM: 1.358 | N=23303 | Slope: +0.38]\n",
      "Diag L3-2                                                   ×1.06↗                   ×1.01↗                                                                                                                             \n",
      "L tot=3                                         1.117   ×1.38→ (<29=66>6)   1.545   ×1.23→ (<34=45>22)   1.904        V                                                                                               [GM: 1.486 | N=3342 | Slope: +0.39]\n",
      "Diag L4-3                             ×1.01↗                   ×1.06↗                   ×0.91↗                                                                                                                             \n",
      "L tot=4                   1.111   ×1.31→ (<27=66>7)   1.456   ×1.43→ (<40=45>15)   2.086   ×0.96→ (<32=33>36)   2.011        V                                                                                               [GM: 1.614 | N=347 | Slope: +0.33]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "M Diag Left     ×1.05↗        1.17       ×1.04↗        1.52       ×0.91↗        1.99                                                                                                                                       \n",
      "M Vert Left               1.111       ×1.15→       1.275       ×1.22→       1.561       ×1.06→       1.661                                                                                                                 \n",
      "Agg L First→                                                                                                                                                                                             [(<29.1=59.8>11.1) N=26992]\n"
     ]
    }
   ],
   "source": [
    "# Print full table to see both aggregate rows\n",
    "print(table_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df37fff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disorder metrics available for 185 languages\n",
      "Right extreme disorder range: 0.0% - 1.0%\n",
      "Left extreme disorder range: 0.3% - 1.0%\n"
     ]
    }
   ],
   "source": [
    "# Disorder metrics already extracted in previous cell during mass table generation\n",
    "# Check if disorder_df exists and display summary\n",
    "if 'disorder_df' in locals() and disorder_df is not None:\n",
    "    print(f\"Disorder metrics available for {len(disorder_df)} languages\")\n",
    "    print(f\"Right extreme disorder range: {disorder_df['right_extreme_disorder'].min():.1f}% - {disorder_df['right_extreme_disorder'].max():.1f}%\")\n",
    "    print(f\"Left extreme disorder range: {disorder_df['left_extreme_disorder'].min():.1f}% - {disorder_df['left_extreme_disorder'].max():.1f}%\")\n",
    "else:\n",
    "    print(\"No disorder metrics available. Run the mass table generation cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f33c248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating all disorder and diagonal factor plots. This takes a staggering 2 minutes 10...\n",
      "Merged data: 185 languages with disorder and VO data\n",
      "Generating 6 plots in parallel using 6 workers...\n",
      "  ✓ Plot 1: 121 languages -> right_extreme_disorder_vs_vo.png\n",
      "  ✓ Plot 2: 159 languages -> left_extreme_disorder_vs_vo.png\n",
      "  ✓ Plot 3: 120 languages -> right_extreme_diag_factor_vs_vo.png\n",
      "  ✗ Plot 4: No valid data\n",
      "  ✓ Plot 5: 120 languages -> right_diag_factor_vs_disorder.png\n",
      "  ✗ Plot 6: No valid data\n",
      "\n",
      "============================================================\n",
      "Successfully created 4 plots:\n",
      "  ✓ plots/right_extreme_disorder_vs_vo.png\n",
      "  ✓ plots/left_extreme_disorder_vs_vo.png\n",
      "  ✓ plots/right_extreme_diag_factor_vs_vo.png\n",
      "  ✓ plots/right_diag_factor_vs_disorder.png\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "## Plot All Disorder Metrics and Diagonal Factors\n",
    "# Create comprehensive scatter plots comparing:\n",
    "# - Disorder percentages vs VO scores\n",
    "# - Diagonal growth factors vs VO scores  \n",
    "# - Diagonal factors vs disorder percentages (right and left)\n",
    "\n",
    "import plotting\n",
    "from importlib import reload\n",
    "reload(plotting)\n",
    "\n",
    "if 'disorder_df' in locals() and disorder_df is not None:\n",
    "    print(\"Generating all disorder and diagonal factor plots. This takes a staggering 2 minutes 10...\")\n",
    "    \n",
    "    saved_plots = plotting.plot_disorder_metrics_vs_vo(\n",
    "        disorder_df=disorder_df,\n",
    "        langnameGroup=langnameGroup,\n",
    "        appearance_dict=metadata['appearance_dict'],\n",
    "        data_dir=DATA_DIR,\n",
    "        plots_dir='plots'\n",
    "    )\n",
    "    \n",
    "    if saved_plots:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Successfully created {len(saved_plots)} plots:\")\n",
    "        for plot_path in saved_plots:\n",
    "            print(f\"  ✓ {plot_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(\"No plots were created. Check error messages above.\")\n",
    "else:\n",
    "    print(\"Cannot create plots: disorder_df not available. Run the mass table generation cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760de926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3165d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c36863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
