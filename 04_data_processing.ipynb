{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# 04 Data Processing and Factor Computation\n",
        "\n",
        "This notebook loads the processed dependency size data, calculates linguistic factors (such as HCS and Diagonal factors), and generates analysis tables.\n",
        "\n",
        "**Output**:\n",
        "- `data/hcs_factors.csv`: Computed HCS factors for all languages.\n",
        "- `data/verb_centered_table.txt`: Verb-centered constituent size table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9f011922",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'verb_centered_analysis' from '/bigstorage/kim/typometrics/dataanalysis/verb_centered_analysis.py'>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from importlib import reload\n",
        "\n",
        "# Custom modules\n",
        "import data_utils\n",
        "import compute_factors\n",
        "import verb_centered_analysis\n",
        "\n",
        "# Reload to ensure latest changes are picked up\n",
        "reload(compute_factors)\n",
        "reload(verb_centered_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_DIR = \"data\"\n",
        "OUTPUT_DIR = \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_data",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "load_metadata",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded metadata from data/metadata.pkl\n",
            "Loaded metadata for 186 languages\n"
          ]
        }
      ],
      "source": [
        "metadata = data_utils.load_metadata(os.path.join(DATA_DIR, 'metadata.pkl'))\n",
        "langNames = metadata['langNames']\n",
        "langnameGroup = metadata['langnameGroup']\n",
        "\n",
        "print(f\"Loaded metadata for {len(langNames)} languages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "load_metrics",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded average sizes for 185 languages\n"
          ]
        }
      ],
      "source": [
        "# Load average sizes\n",
        "with open(os.path.join(DATA_DIR, 'all_langs_average_sizes.pkl'), 'rb') as f:\n",
        "    all_langs_average_sizes_filtered = pickle.load(f)\n",
        "\n",
        "# Save filtered data for notebook 05 (if needed by downstream)\n",
        "with open(os.path.join(DATA_DIR, 'all_langs_average_sizes_filtered.pkl'), 'wb') as f:\n",
        "    pickle.dump(all_langs_average_sizes_filtered, f)\n",
        "\n",
        "print(f\"Loaded average sizes for {len(all_langs_average_sizes_filtered)} languages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hcs_factors",
      "metadata": {},
      "source": [
        "## 2. Compute HCS Factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "compute_hcs",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed HCS factors for 170 languages\n",
            "    language_code  language_name           group  right_1_totright_2  \\\n",
            "80             ko         Korean           Other            1.824906   \n",
            "116           pad        Paumarí  South-American            2.139826   \n",
            "146            tn         Tswana     Niger-Congo            2.213364   \n",
            "137           ssp    SpanishSign   Indo-European            1.525921   \n",
            "144           qte  TeluguEnglish       Dravidian            1.000000   \n",
            "\n",
            "     right_2_totright_2  hcs_factor  \n",
            "80             1.031262    0.565104  \n",
            "116            1.414214    0.660901  \n",
            "146            1.778279    0.803428  \n",
            "137            1.389013    0.910278  \n",
            "144            1.000000    1.000000  \n"
          ]
        }
      ],
      "source": [
        "hcs_df = compute_factors.compute_hcs_factors(\n",
        "    all_langs_average_sizes_filtered, \n",
        "    langNames, \n",
        "    langnameGroup\n",
        ")\n",
        "\n",
        "print(f\"Computed HCS factors for {len(hcs_df)} languages\")\n",
        "print(hcs_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "save_hcs",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved HCS factors to data/hcs_factors.csv\n"
          ]
        }
      ],
      "source": [
        "hcs_path = os.path.join(OUTPUT_DIR, 'hcs_factors.csv')\n",
        "hcs_df.to_csv(hcs_path, index=False)\n",
        "print(f\"Saved HCS factors to {hcs_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verb_centered",
      "metadata": {},
      "source": [
        "## 3. Helix tables (Verb-Centered Constituent Size Analysis)\n",
        "\n",
        "Verb-Centered Constituent Size Analysis = a helix table with consituent size averages X per construction of dependents to the right and the left of the verb: \n",
        "      VXXXX\n",
        "      VXXX\n",
        "      VXX\n",
        "      VX\n",
        "     XV\n",
        "    XXV\n",
        "   XXXV\n",
        "  XXXXV\n",
        "\n",
        "This table shold come in multiple options:\n",
        "\n",
        "1. Simple: just showing the average constituent size X per construction of dependents to the right and the left of the verb\n",
        "2. Horizontal Growth: inbetween two Xs, the factor of growth going from left to right\n",
        "3. Horizontal Growth: inbetween two Xs, the factor of growth going from right to left\n",
        "4. Diagonal Growth added as an extra line: Going up right, the growth factor going between the last X of one to the next constructio, such as between the last X of VXXX to the last X of VXXXX etc.\n",
        "5. The same as 4 but going down left.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "compute_table",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verb-Centered Constituent Size Analysis\n",
        "position_averages = verb_centered_analysis.compute_average_sizes_table(all_langs_average_sizes_filtered)\n",
        "# Table saved in mass generation step below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c456c761",
      "metadata": {},
      "source": [
        "## 4. Generate Helix Tables (Mass Generation)\n",
        "\n",
        "Generate helix tables for all languages, families, and order types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4d8625d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## OPTIONAL: Compute Disorder Percentages\n",
        "# ## Uncomment to compute disorder statistics\n",
        "\n",
        "# import compute_disorder\n",
        "# from importlib import reload\n",
        "# reload(compute_disorder)\n",
        "\n",
        "# # Compute disorder statistics with granular stats if available\n",
        "# import pickle\n",
        "\n",
        "# # Load granular ordering stats\n",
        "# ordering_stats = {}\n",
        "# disorder_pct_path = 'data/sentence_disorder_percentages.pkl'\n",
        "# if os.path.exists(disorder_pct_path):\n",
        "#     with open(disorder_pct_path, 'rb') as f:\n",
        "#         ordering_stats = pickle.load(f)\n",
        "#     print(f\"Loaded granular ordering stats for {len(ordering_stats)} languages\")\n",
        "\n",
        "# disorder_df, disorder_percentages = compute_disorder.compute_disorder_statistics(\n",
        "#     all_langs_average_sizes_filtered,\n",
        "#     langNames,\n",
        "#     langnameGroup,\n",
        "#     ordering_stats=ordering_stats\n",
        "# )\n",
        "# print(f\"Computed disorder for {len(disorder_df)} languages\")\n",
        "# print(\"\\nDisorder percentages by configuration:\")\n",
        "# for (side, tot), pct in sorted(disorder_percentages.items()):\n",
        "#     if pct is not None:\n",
        "#         print(f\"  {side} tot={tot}: {pct:.1f}% disordered\")\n",
        "\n",
        "# print(\"\\n\", disorder_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7bfb224a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Mass Table Generation ---\n",
            "Loaded Ordering Stats (Triplets) successfully.\n",
            "Loaded VO/OV classifications.\n",
            "Generating tables in data/tables...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mass table generation complete. Output in: data/tables\n",
            "--- Completed. Tables saved to data/tables/ ---\n",
            "\n",
            "Extracted and saved disorder metrics for 185 languages to data/disorder_extreme_aggregates.csv\n",
            "\n",
            "Sample disorder metrics:\n",
            "  language_code  language_name  right_extreme_disorder  left_extreme_disorder  \\\n",
            "0           abq          Abaza               25.000000              21.052632   \n",
            "1            ab         Abkhaz                0.000000              17.148362   \n",
            "2            af      Afrikaans               19.103314              45.288501   \n",
            "3           aqz        Akuntsu                0.000000               3.846154   \n",
            "4            sq       Albanian               10.828025              14.583333   \n",
            "5           gsw    SwissGerman                9.056604              36.990291   \n",
            "6            am        Amharic                3.217158              22.857143   \n",
            "7           grc   AncientGreek               18.419401              31.912067   \n",
            "8           hbo  AncientHebrew               16.675589              16.451234   \n",
            "9           apu        Apurinã                8.000000              15.555556   \n",
            "\n",
            "   right_extreme_diag_factor  left_extreme_diag_factor  \n",
            "0                   0.869579                  1.054106  \n",
            "1                   1.122899                  1.074071  \n",
            "2                   1.057606                  1.007251  \n",
            "3                   1.032774                  1.141716  \n",
            "4                   1.320898                  1.042484  \n",
            "5                   0.996007                  1.008661  \n",
            "6                   0.995252                  1.306125  \n",
            "7                   1.158881                  0.965586  \n",
            "8                   1.190245                  1.059297  \n",
            "9                   1.057665                  1.072730  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from importlib import reload\n",
        "import verb_centered_analysis\n",
        "\n",
        "# Reload the module to use the latest table formatting logic\n",
        "reload(verb_centered_analysis)\n",
        "\n",
        "OUTPUT_TABLE_DIR = os.path.join(DATA_DIR, 'tables')\n",
        "\n",
        "print(\"--- Starting Mass Table Generation ---\")\n",
        "\n",
        "# 1. Load Average Sizes (should already be loaded from previous cells)\n",
        "if 'all_langs_average_sizes_filtered' not in locals():\n",
        "    avg_path = os.path.join(DATA_DIR, 'all_langs_average_sizes_filtered.pkl')\n",
        "    if os.path.exists(avg_path):\n",
        "        with open(avg_path, 'rb') as f:\n",
        "            all_langs_average_sizes_filtered = pickle.load(f)\n",
        "        print(\"Loaded average sizes from disk.\")\n",
        "    else:\n",
        "        print(\"ERROR: 'all_langs_average_sizes_filtered' not found. Run previous cells first.\")\n",
        "\n",
        "# 2. Load Ordering Statistics (optional - for ordering triples)\n",
        "ordering_stats = {}\n",
        "disorder_path = os.path.join(DATA_DIR, 'sentence_disorder_percentages.pkl')\n",
        "\n",
        "if os.path.exists(disorder_path):\n",
        "    with open(disorder_path, 'rb') as f:\n",
        "        loaded_data = pickle.load(f)\n",
        "        \n",
        "    # Check if the data is in the new format (triplet counts)\n",
        "    sample_lang = next(iter(loaded_data))\n",
        "    sample_keys = list(loaded_data[sample_lang].keys()) if loaded_data[sample_lang] else []\n",
        "    \n",
        "    if sample_keys and len(sample_keys[0]) == 3:\n",
        "        print(\"Loaded Ordering Stats (Triplets) successfully.\")\n",
        "        ordering_stats = loaded_data\n",
        "    else:\n",
        "        print(\"WARNING: Loaded data seems to be in OLD format. Triples cannot be shown.\")\n",
        "else:\n",
        "    print(f\"WARNING: {disorder_path} not found. Tables will be generated without triples.\")\n",
        "\n",
        "# 3. Load VO/OV Data (optional - for order-based tables)\n",
        "vo_data = {}\n",
        "vo_path = os.path.join(DATA_DIR, 'vo_vs_hi_scores.csv')\n",
        "if os.path.exists(vo_path):\n",
        "    vo_df = pd.read_csv(vo_path)\n",
        "    for _, row in vo_df.iterrows():\n",
        "        vo_data[row['language_code']] = row.to_dict()\n",
        "    print(\"Loaded VO/OV classifications.\")\n",
        "\n",
        "# 4. Generate Tables AND Extract Disorder Metrics\n",
        "# This will generate: Global, Individual, Family-based, and Order-based tables.\n",
        "# Also extracts disorder extreme aggregate percentages for each language\n",
        "disorder_df = verb_centered_analysis.generate_mass_tables(\n",
        "    all_langs_average_sizes_filtered,\n",
        "    ordering_stats,\n",
        "    metadata,  # Ensure metadata is loaded (from Cell 6)\n",
        "    vo_data=vo_data,\n",
        "    output_dir=OUTPUT_TABLE_DIR,\n",
        "    arrow_direction='left_to_right',\n",
        "    extract_disorder_metrics=True\n",
        ")\n",
        "\n",
        "print(f\"--- Completed. Tables saved to {OUTPUT_TABLE_DIR}/ ---\")\n",
        "\n",
        "# Save disorder metrics if extracted\n",
        "if disorder_df is not None and len(disorder_df) > 0:\n",
        "    disorder_csv_path = os.path.join(DATA_DIR, 'disorder_extreme_aggregates.csv')\n",
        "    disorder_df.to_csv(disorder_csv_path, index=False)\n",
        "    print(f\"\\nExtracted and saved disorder metrics for {len(disorder_df)} languages to {disorder_csv_path}\")\n",
        "    print(\"\\nSample disorder metrics:\")\n",
        "    print(disorder_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6769ec72",
      "metadata": {},
      "source": [
        "## 5. Conll Configuration Example Creation\n",
        "\n",
        "Generate interactive HTML visualizations of verb configurations from examples collected during data extraction.\n",
        "\n",
        "**Note**: Examples are automatically collected by `run_data_extraction.py` using the same constraints as constituent size computation (same dependency relations, bastard inclusion, etc.). This ensures consistency and avoids duplicate CoNLL file parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4831de51",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating HTML visualizations from saved configuration examples...\n",
            "============================================================\n",
            "Loading configuration examples...\n",
            "Loading metadata...\n",
            "Loading position counts...\n",
            "Loaded position counts for 185 languages\n",
            "Generating HTML for 185 languages...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating HTML files: 100%|██████████| 185/185 [00:05<00:00, 35.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating index...\n",
            "Generated index at html_examples/index.html\n",
            "Done! HTML files saved to html_examples/\n",
            "\n",
            "============================================================\n",
            "Configuration Examples Generated Successfully!\n",
            "============================================================\n",
            "Output directory: html_examples/\n",
            "Open html_examples/index.html to browse examples\n",
            "\n",
            "Features:\n",
            "  • Interactive dependency trees with reactive-dep-tree\n",
            "  • Verbs highlighted in red, dependents in green\n",
            "  • Organized by language with 3-column layout\n",
            "  • Same constraints as constituent size computation\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate HTML visualizations from configuration examples collected during data extraction\n",
        "# Examples are automatically collected in run_data_extraction.py using the same constraints\n",
        "# as constituent size computation (same dependency relations, bastard inclusion, etc.)\n",
        "\n",
        "import generate_html_examples\n",
        "from importlib import reload\n",
        "reload(generate_html_examples)\n",
        "\n",
        "print(\"Generating HTML visualizations from saved configuration examples...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check if examples have been collected\n",
        "examples_path = os.path.join(DATA_DIR, 'all_config_examples.pkl')\n",
        "if not os.path.exists(examples_path):\n",
        "    print(f\"ERROR: Configuration examples not found at {examples_path}\")\n",
        "    print(\"\\nPlease run data extraction first:\")\n",
        "    print(\"  python3 run_data_extraction.py\")\n",
        "    print(\"\\nThis will collect examples during the main processing pipeline.\")\n",
        "else:\n",
        "    # Generate HTML from saved examples\n",
        "    output_dir = 'html_examples'\n",
        "    generate_html_examples.generate_all_html(\n",
        "        data_dir=DATA_DIR,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Configuration Examples Generated Successfully!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Output directory: {output_dir}/\")\n",
        "    print(f\"Open {output_dir}/index.html to browse examples\")\n",
        "    print(f\"\\nFeatures:\")\n",
        "    print(f\"  • Interactive dependency trees with reactive-dep-tree\")\n",
        "    print(f\"  • Verbs highlighted in red, dependents in green\")\n",
        "    print(f\"  • Organized by language with 3-column layout\")\n",
        "    print(f\"  • Same constraints as constituent size computation\")\n",
        "    print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f702c720",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using language: en\n",
            "Diag R2-1                                                                                                                     ×1.11↗                                                                                 \n",
            "R tot=1                                                                                                  V        3.349                                                                                  [GM: 3.349 | N=72090]\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "X V X                                                                                       1.258   ×2.80→ (<74=20>6)   3.519                                                                                           \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "L tot=1                                                                                     1.273        V                                                                                               [GM: 1.273 | N=54183]\n",
            "Diag L2-1                                                                         ×1.08↗                                                                                                                             \n",
            "L tot=2                                                               1.180   ×1.32→ (<28=62>9)   1.563        V                                                                                               [GM: 1.358 | N=23303 | Slope: +0.38]\n",
            "Diag L3-2                                                   ×1.06↗                   ×1.01↗                                                                                                                             \n",
            "L tot=3                                         1.117   ×1.38→ (<29=66>6)   1.545   ×1.23→ (<34=45>22)   1.904        V                                                                                               [GM: 1.486 | N=3342 | Slope: +0.39]\n",
            "Diag L4-3                             ×1.01↗                   ×1.06↗                   ×0.91↗                                                                                                                             \n",
            "L tot=4                   1.111   ×1.31→ (<27=66>7)   1.456   ×1.43→ (<40=45>15)   2.086   ×0.96→ (<32=33>36)   2.011        V                                                                                               [GM: 1.614 | N=347 | Slope: +0.33]\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "M Diag Left     ×1.05↗        1.17       ×1.04↗        1.52       ×0.91↗        1.99                                                                                                                                       \n",
            "M Vert Left               1.111       ×1.15→       1.275       ×1.22→       1.561       ×1.06→       1.661                                                                                                                 \n",
            "Agg L First→                                                                                                                                                                                             [(<29.1=59.8>11.1) N=26992]\n"
          ]
        }
      ],
      "source": [
        "# Test updated table with all fixes\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Reload all verb-centered modules\n",
        "if 'verb_centered_model' in sys.modules:\n",
        "    importlib.reload(sys.modules['verb_centered_model'])\n",
        "if 'verb_centered_computations' in sys.modules:\n",
        "    importlib.reload(sys.modules['verb_centered_computations'])\n",
        "if 'verb_centered_layout' in sys.modules:\n",
        "    importlib.reload(sys.modules['verb_centered_layout'])\n",
        "if 'verb_centered_builder' in sys.modules:\n",
        "    importlib.reload(sys.modules['verb_centered_builder'])\n",
        "if 'verb_centered_formatters' in sys.modules:\n",
        "    importlib.reload(sys.modules['verb_centered_formatters'])\n",
        "if 'verb_centered_analysis' in sys.modules:\n",
        "    importlib.reload(sys.modules['verb_centered_analysis'])\n",
        "\n",
        "from verb_centered_analysis import create_verb_centered_table\n",
        "from verb_centered_model import TableConfig\n",
        "from verb_centered_formatters import TextTableFormatter\n",
        "\n",
        "config_debug = TableConfig(\n",
        "    show_horizontal_factors=True,\n",
        "    show_diagonal_factors=True,\n",
        "    show_ordering_triples=True,\n",
        "    show_row_averages=True,\n",
        "    show_marginal_means=True,\n",
        "    arrow_direction='left_to_right'\n",
        ")\n",
        "\n",
        "# For testing, we need language-specific data\n",
        "# Get English data if available\n",
        "test_lang = 'en'  # Try different language codes\n",
        "english_position_avg = None\n",
        "english_ordering = None\n",
        "\n",
        "for lang_code in all_langs_average_sizes_filtered.keys():\n",
        "    if lang_code.startswith('en'):\n",
        "        english_position_avg = all_langs_average_sizes_filtered[lang_code]\n",
        "        english_ordering = ordering_stats.get(lang_code, {})\n",
        "        print(f\"Using language: {lang_code}\")\n",
        "        break\n",
        "\n",
        "# Fallback to global if no English found\n",
        "if english_position_avg is None:\n",
        "    print(\"No English data found, using global averages (no ordering triples)\")\n",
        "    english_position_avg = position_averages\n",
        "    english_ordering = None\n",
        "\n",
        "# Create table structure\n",
        "table_struct = create_verb_centered_table(\n",
        "    position_averages=english_position_avg,\n",
        "    ordering_stats=english_ordering,\n",
        "    hcs_row=None,\n",
        "    config=config_debug,\n",
        "    output_format='struct'\n",
        ")\n",
        "\n",
        "# Format as text\n",
        "formatter = TextTableFormatter(table_struct)\n",
        "table_txt = formatter.format()\n",
        "\n",
        "# Print only lower half to check\n",
        "lines = table_txt.split('\\n')\n",
        "print('\\n'.join(lines[10:]))  # Skip upper half to focus on lower half"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b26681c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No English ordering stats found\n",
            "Available languages: ['abq', 'ab', 'af', 'aqz', 'sq']\n"
          ]
        }
      ],
      "source": [
        "# Check ordering_stats for English\n",
        "if 'en_ewt' in ordering_stats:\n",
        "    print(\"English ordering stats found:\")\n",
        "    for key, value in sorted(ordering_stats['en_ewt'].items()):\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"No English ordering stats found\")\n",
        "    print(f\"Available languages: {list(ordering_stats.keys())[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2030de2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "position_averages type: <class 'dict'>\n",
            "First few keys: ['left_1', 'left_1_totleft_1', 'average_totleft_1', 'left_2', 'left_2_totleft_2', 'left_1_totleft_2', 'average_totleft_2', 'right_1', 'right_1_totright_1', 'average_totright_1']\n"
          ]
        }
      ],
      "source": [
        "# Check what position_averages contains\n",
        "print(\"position_averages type:\", type(position_averages))\n",
        "if isinstance(position_averages, dict):\n",
        "    print(\"First few keys:\", list(position_averages.keys())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "197e719e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row                         L4                    L3                    L2                    L1         V          R1                    R2                    R3                    R4                 [GM | N | Slope]\n",
            "M Vert Right                                                                                                      2.032                 2.765                 3.698                 5.257                         \n",
            "M Diag Right                                                                                                                             1.99       ×1.33↗        2.33       ×1.31↗        4.17       ×1.16↗               \n",
            "Agg R Last→                                                                                                                                                                                              [(<68.4=16.4>15.2) N=33542]\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "R tot=4                                                                                                  V        1.712   ×1.34→ (<47=32>21)   2.288   ×1.29→ (<50=22>29)   2.953   ×1.78→ (<66=14>21)   5.257                [GM: 2.792 | N=546 | Slope: +1.13]\n",
            "Diag R4-3                                                                                                                     ×1.33↗                   ×1.19↗                   ×1.14↗                                     \n",
            "R tot=3                                                                                                  V        1.726   ×1.44→ (<53=27>20)   2.491   ×1.86→ (<69=13>18)   4.631                                      [GM: 2.710 | N=4557 | Slope: +1.45]\n",
            "Diag R3-2                                                                                                                     ×1.44↗                   ×1.25↗                                                           \n",
            "R tot=2                                                                                                  V        1.724   ×2.15→ (<68=17>15)   3.709                                                            [GM: 2.529 | N=28439 | Slope: +1.98]\n",
            "Diag R2-1                                                                                                                     ×1.11↗                                                                                 \n",
            "R tot=1                                                                                                  V        3.349                                                                                  [GM: 3.349 | N=72090]\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "X V X                                                                                       1.258   ×2.80→ (<74=20>6)   3.519                                                                                           \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "L tot=1                                                                                     1.273        V                                                                                               [GM: 1.273 | N=54183]\n",
            "Diag L2-1                                                                         ×1.08↗                                                                                                                             \n",
            "L tot=2                                                               1.180   ×1.32→ (<28=62>9)   1.563        V                                                                                               [GM: 1.358 | N=23303 | Slope: +0.38]\n",
            "Diag L3-2                                                   ×1.06↗                   ×1.01↗                                                                                                                             \n",
            "L tot=3                                         1.117   ×1.38→ (<29=66>6)   1.545   ×1.23→ (<34=45>22)   1.904        V                                                                                               [GM: 1.486 | N=3342 | Slope: +0.39]\n",
            "Diag L4-3                             ×1.01↗                   ×1.06↗                   ×0.91↗                                                                                                                             \n",
            "L tot=4                   1.111   ×1.31→ (<27=66>7)   1.456   ×1.43→ (<40=45>15)   2.086   ×0.96→ (<32=33>36)   2.011        V                                                                                               [GM: 1.614 | N=347 | Slope: +0.33]\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "M Diag Left     ×1.05↗        1.17       ×1.04↗        1.52       ×0.91↗        1.99                                                                                                                                       \n",
            "M Vert Left               1.111       ×1.15→       1.275       ×1.22→       1.561       ×1.06→       1.661                                                                                                                 \n",
            "Agg L First→                                                                                                                                                                                             [(<29.1=59.8>11.1) N=26992]\n"
          ]
        }
      ],
      "source": [
        "# Print full table to see both aggregate rows\n",
        "print(table_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "df37fff0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Disorder metrics available for 185 languages\n",
            "Right extreme disorder range: 0.0% - 81.0%\n",
            "Left extreme disorder range: 0.0% - 59.5%\n"
          ]
        }
      ],
      "source": [
        "# Disorder metrics already extracted in previous cell during mass table generation\n",
        "# Check if disorder_df exists and display summary\n",
        "if 'disorder_df' in locals() and disorder_df is not None:\n",
        "    print(f\"Disorder metrics available for {len(disorder_df)} languages\")\n",
        "    print(f\"Right extreme disorder range: {disorder_df['right_extreme_disorder'].min():.1f}% - {disorder_df['right_extreme_disorder'].max():.1f}%\")\n",
        "    print(f\"Left extreme disorder range: {disorder_df['left_extreme_disorder'].min():.1f}% - {disorder_df['left_extreme_disorder'].max():.1f}%\")\n",
        "else:\n",
        "    print(\"No disorder metrics available. Run the mass table generation cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f33c248",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating all disorder and diagonal factor plots. This takes a staggering 5 minutes 30...\n",
            "Merged data: 185 languages with disorder and VO data\n",
            "Generating 6 plots in parallel using 6 workers...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Plot 1: 171 languages -> right_extreme_disorder_vs_vo.png\n",
            "  ✓ Plot 2: 185 languages -> left_extreme_disorder_vs_vo.png\n",
            "  ✓ Plot 3: 170 languages -> right_extreme_diag_factor_vs_vo.png\n",
            "  ✓ Plot 4: 185 languages -> left_extreme_diag_factor_vs_vo.png\n",
            "  ✓ Plot 5: 170 languages -> right_diag_factor_vs_disorder.png\n",
            "  ✓ Plot 6: 185 languages -> left_diag_factor_vs_disorder.png\n",
            "\n",
            "============================================================\n",
            "Successfully created 6 plots:\n",
            "  ✓ plots/right_extreme_disorder_vs_vo.png\n",
            "  ✓ plots/left_extreme_disorder_vs_vo.png\n",
            "  ✓ plots/right_extreme_diag_factor_vs_vo.png\n",
            "  ✓ plots/left_extreme_diag_factor_vs_vo.png\n",
            "  ✓ plots/right_diag_factor_vs_disorder.png\n",
            "  ✓ plots/left_diag_factor_vs_disorder.png\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "## Plot All Disorder Metrics and Diagonal Factors\n",
        "# Create comprehensive scatter plots comparing:\n",
        "# - Disorder percentages vs VO scores\n",
        "# - Diagonal growth factors vs VO scores  \n",
        "# - Diagonal factors vs disorder percentages (right and left)\n",
        "\n",
        "import plotting\n",
        "from importlib import reload\n",
        "reload(plotting)\n",
        "\n",
        "if 'disorder_df' in locals() and disorder_df is not None:\n",
        "    print(\"Generating all disorder and diagonal factor plots. This takes a staggering 2 minutes 10...\")\n",
        "    \n",
        "    saved_plots = plotting.plot_disorder_metrics_vs_vo(\n",
        "        disorder_df=disorder_df,\n",
        "        langnameGroup=langnameGroup,\n",
        "        appearance_dict=metadata['appearance_dict'],\n",
        "        data_dir=DATA_DIR,\n",
        "        plots_dir='plots'\n",
        "    )\n",
        "    \n",
        "    if saved_plots:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Successfully created {len(saved_plots)} plots:\")\n",
        "        for plot_path in saved_plots:\n",
        "            print(f\"  ✓ {plot_path}\")\n",
        "        print(f\"{'='*60}\")\n",
        "    else:\n",
        "        print(\"No plots were created. Check error messages above.\")\n",
        "else:\n",
        "    print(\"Cannot create plots: disorder_df not available. Run the mass table generation cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760de926",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f3165d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c36863",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ragenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
